{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Document Fraud Detection — EfficientNet-B3 Training on Colab\n",
    "\n",
    "Trains on **CASIA v2.0** (~7,491 genuine + 5,123 tampered) and **COVERAGE** (100+100) datasets.\n",
    "Expected result: val AUC > 0.90 by epoch 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: GPU check\n",
    "import torch\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'VRAM: {total_mem:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU detected. Go to Runtime > Change runtime type > GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install dependencies\n",
    "!pip install -q torch torchvision scipy scikit-learn loguru PyMuPDF opencv-python-headless pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Mount Google Drive (for saving model checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_SAVE_PATH = '/content/drive/MyDrive/fraud_detection_models'\n",
    "os.makedirs(DRIVE_SAVE_PATH, exist_ok=True)\n",
    "print(f'Model will be saved to: {DRIVE_SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-casia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Download CASIA v2.0 via Kaggle API\n",
    "# Step 1: Upload your kaggle.json file\n",
    "from google.colab import files\n",
    "print('Upload your kaggle.json API key file:')\n",
    "uploaded = files.upload()  # Upload kaggle.json\n",
    "\n",
    "import os\n",
    "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download CASIA v2.0\n",
    "# Dataset: https://www.kaggle.com/datasets/divg07/casia-20-image-tampering-detection-dataset\n",
    "!kaggle datasets download -d divg07/casia-20-image-tampering-detection-dataset -p /content/casia_raw\n",
    "!unzip -q /content/casia_raw/*.zip -d /content/casia_raw/\n",
    "print('CASIA v2.0 downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Clone COVERAGE dataset (100 genuine + 100 tampered)\n",
    "!git clone https://github.com/wenbihan/coverage.git /content/coverage_raw\n",
    "\n",
    "import os\n",
    "coverage_image_dir = '/content/coverage_raw/image'\n",
    "genuine_files = [f for f in os.listdir(coverage_image_dir)\n",
    "                 if f.endswith('.tif') and not f[:-4].endswith('f')]\n",
    "tampered_files = [f for f in os.listdir(coverage_image_dir)\n",
    "                  if f.endswith('.tif') and f[:-4].endswith('f')]\n",
    "print(f'COVERAGE: {len(genuine_files)} genuine, {len(tampered_files)} tampered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-prepare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset preparation — stratified 75/12.5/12.5 train/val/test split\n",
    "import os, shutil, random\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = '/content/dataset'\n",
    "\n",
    "def prepare_dataset(casia_au_dir, casia_tp_dir, coverage_image_dir, output_dir,\n",
    "                    train_ratio=0.75, val_ratio=0.125):\n",
    "    \"\"\"Build stratified split dataset from CASIA v2.0 + COVERAGE.\"\"\"\n",
    "    valid_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n",
    "\n",
    "    def collect(directory):\n",
    "        return [os.path.join(directory, f) for f in os.listdir(directory)\n",
    "                if Path(f).suffix.lower() in valid_exts]\n",
    "\n",
    "    genuine_files = collect(casia_au_dir)\n",
    "    tampered_files = collect(casia_tp_dir)\n",
    "\n",
    "    # Add COVERAGE images\n",
    "    for f in os.listdir(coverage_image_dir):\n",
    "        p = os.path.join(coverage_image_dir, f)\n",
    "        if Path(f).suffix.lower() == '.tif':\n",
    "            base = Path(f).stem\n",
    "            if base.endswith('f'):\n",
    "                tampered_files.append(p)\n",
    "            else:\n",
    "                genuine_files.append(p)\n",
    "\n",
    "    print(f'Total: {len(genuine_files)} genuine, {len(tampered_files)} tampered')\n",
    "\n",
    "    def split_files(files):\n",
    "        random.shuffle(files)\n",
    "        n = len(files)\n",
    "        n_train = int(n * train_ratio)\n",
    "        n_val = int(n * val_ratio)\n",
    "        return files[:n_train], files[n_train:n_train+n_val], files[n_train+n_val:]\n",
    "\n",
    "    g_train, g_val, g_test = split_files(genuine_files)\n",
    "    t_train, t_val, t_test = split_files(tampered_files)\n",
    "\n",
    "    splits = {\n",
    "        'train': {'genuine': g_train, 'tampered': t_train},\n",
    "        'val':   {'genuine': g_val,   'tampered': t_val},\n",
    "        'test':  {'genuine': g_test,  'tampered': t_test},\n",
    "    }\n",
    "\n",
    "    for split, classes in splits.items():\n",
    "        for cls, files in classes.items():\n",
    "            dest_dir = os.path.join(output_dir, split, cls)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            for src in files:\n",
    "                shutil.copy2(src, os.path.join(dest_dir, os.path.basename(src)))\n",
    "        g_count = len(classes['genuine'])\n",
    "        t_count = len(classes['tampered'])\n",
    "        print(f'  {split}: {g_count} genuine, {t_count} tampered')\n",
    "\n",
    "    print(f'Dataset prepared at {output_dir}')\n",
    "\n",
    "# Adjust paths based on actual CASIA zip structure after extraction\n",
    "CASIA_AU  = '/content/casia_raw/Au'\n",
    "CASIA_TP  = '/content/casia_raw/Tp'\n",
    "COVERAGE_IMG = '/content/coverage_raw/image'\n",
    "\n",
    "random.seed(42)\n",
    "prepare_dataset(CASIA_AU, CASIA_TP, COVERAGE_IMG, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Clone or upload project code\n",
    "# Option A: clone from GitHub (replace with your repo URL)\n",
    "# !git clone https://github.com/YOUR_USERNAME/document_fraud_ai.git /content/document_fraud_ai\n",
    "\n",
    "# Option B: upload the project zip\n",
    "from google.colab import files\n",
    "print('Upload your document_fraud_ai project zip:')\n",
    "uploaded = files.upload()\n",
    "zip_name = list(uploaded.keys())[0]\n",
    "!unzip -q \"{zip_name}\" -d /content/\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/document_fraud_ai')\n",
    "print('Project code ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train EfficientNet-B3\n",
    "!cd /content/document_fraud_ai && python train_model.py \\\n",
    "    --data_dir /content/dataset \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 16 \\\n",
    "    --freeze_epochs 5 \\\n",
    "    --patience 10 \\\n",
    "    --output_dir /content/document_fraud_ai/fraud_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Copy trained model to Google Drive\n",
    "import shutil, os\n",
    "\n",
    "model_src = '/content/document_fraud_ai/fraud_model/fraud_efficientnet_b3_best.pth'\n",
    "if os.path.exists(model_src):\n",
    "    dest = os.path.join(DRIVE_SAVE_PATH, 'fraud_efficientnet_b3_best.pth')\n",
    "    shutil.copy2(model_src, dest)\n",
    "    size_mb = os.path.getsize(dest) / 1e6\n",
    "    print(f'Model saved to Drive: {dest} ({size_mb:.1f} MB)')\n",
    "else:\n",
    "    print('ERROR: Model file not found. Check training output above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluate on test split\n",
    "import sys, os\n",
    "sys.path.insert(0, '/content/document_fraud_ai')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fraud_model.cnn_model import FraudEfficientNetB3, IMAGENET_MEAN, IMAGENET_STD\n",
    "from train_model import ELADataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = FraudEfficientNetB3(pretrained=False)\n",
    "model_path = '/content/document_fraud_ai/fraud_model/fraud_efficientnet_b3_best.pth'\n",
    "state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_set = ELADataset('/content/dataset', split='test')\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images.to(device)).squeeze(1).cpu().numpy()\n",
    "        all_preds.extend(outputs.tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "binary_preds = (all_preds > 0.5).astype(int)\n",
    "\n",
    "acc  = accuracy_score(all_labels, binary_preds)\n",
    "auc  = roc_auc_score(all_labels, all_preds)\n",
    "f1   = f1_score(all_labels, binary_preds)\n",
    "cm   = confusion_matrix(all_labels, binary_preds)\n",
    "\n",
    "print(f'Test Accuracy : {acc:.4f} ({acc*100:.1f}%)')\n",
    "print(f'Test AUC-ROC  : {auc:.4f}')\n",
    "print(f'Test F1       : {f1:.4f}')\n",
    "print(f'Confusion Matrix:')\n",
    "print(f'  TN={cm[0,0]}  FP={cm[0,1]}')\n",
    "print(f'  FN={cm[1,0]}  TP={cm[1,1]}')"
   ]
  }
 ]
}
